{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e10ade2-fd38-4dcf-a592-450fa3c1fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/17 13:38:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/17 13:38:35 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.119:7077\") \\\n",
    "        .appName(\"LingkaiZhu\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfb345-e799-4a03-92fd-343700ae7a3b",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6c3b4-e27a-43b0-94e3-dba28eb862a8",
   "metadata": {},
   "source": [
    "## Question A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c71c0-e436-4559-8bf5-8038547177e3",
   "metadata": {},
   "source": [
    "A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a706d8-8974-446e-b60d-1edcf336faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines_english = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.en\")\n",
    "print(lines_english.first())\n",
    "lines_english1 = lines_english.map(lambda line: line.split('\\n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443148f-6d38-415f-8e13-0d8e7c610523",
   "metadata": {},
   "source": [
    "A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b397ad9-4b47-4f9a-8a96-ab8a440c3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiederaufnahme der Sitzungsperiode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines_de = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.de\")\n",
    "print(lines_de.first())\n",
    "lines_de1 = lines_de.map(lambda line: line.split('\\n'))\n",
    "line_de_counts = lines_de1.map(lambda w: len(w))\n",
    "total_de_lines = line_de_counts.reduce(add)\n",
    "print(f'total number of lines = {total_de_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad1568-f846-4f61-b29b-cc15b983ea0f",
   "metadata": {},
   "source": [
    "A.1.3 Verify that the line counts are the same for the two languages.\n",
    "In this case, the count of the english transcripts is 1920209, which is equal to its original language's text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d9a3f-4f26-48ee-95ae-8b9d8015d9d3",
   "metadata": {},
   "source": [
    "A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3b27d1-0f3e-43f2-950e-73aad506dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions of the english: 3\n",
      "number of partitions of the original: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"number of partitions of the english:\", lines_english.getNumPartitions())\n",
    "print(\"number of partitions of the original:\", lines_de.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb19ead-7e17-459a-ba1c-14e54af846bb",
   "metadata": {},
   "source": [
    "## Question A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be21ccd-ce6f-4b6b-88c8-fa1532a7f1a7",
   "metadata": {},
   "source": [
    "A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    "\n",
    "● Lowercase the text\n",
    "\n",
    "● Tokenize the text (split on space)\n",
    "\n",
    "Hint: define a function to run in your driver application to avoid writing this code twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d58de0d-3cfb-4821-ac4b-83ada3f46c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "def preprocess(lines):\n",
    "    lowercase_lines = lines.map(lambda line: line.lower())\n",
    "    words = lowercase_lines\\\n",
    "    .flatMap(lambda line: line.split(' '))\\\n",
    "    .flatMap(lambda line: line.split('\\n'))\n",
    "    return lowercase_lines, words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfa47b-0a9f-4dd9-bf7b-6237e93f72ce",
   "metadata": {},
   "source": [
    "A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd95ba21-b220-401a-a2ec-81a3328b30af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resumption of the session', 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'you have requested a debate on this subject in the course of the next few days, during this part-session.', \"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\", \"please rise, then, for this minute' s silence.\", \"(the house rose and observed a minute' s silence)\", 'madam president, on a point of order.', 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.', 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.']\n",
      "----------------------------------------------\n",
      "['wiederaufnahme der sitzungsperiode', 'ich erkläre die am freitag, dem 17. dezember unterbrochene sitzungsperiode des europäischen parlaments für wiederaufgenommen, wünsche ihnen nochmals alles gute zum jahreswechsel und hoffe, daß sie schöne ferien hatten.', 'wie sie feststellen konnten, ist der gefürchtete \"millenium-bug \" nicht eingetreten. doch sind bürger einiger unserer mitgliedstaaten opfer von schrecklichen naturkatastrophen geworden.', 'im parlament besteht der wunsch nach einer aussprache im verlauf dieser sitzungsperiode in den nächsten tagen.', 'heute möchte ich sie bitten - das ist auch der wunsch einiger kolleginnen und kollegen -, allen opfern der stürme, insbesondere in den verschiedenen ländern der europäischen union, in einer schweigeminute zu gedenken.', 'ich bitte sie, sich zu einer schweigeminute zu erheben.', '(das parlament erhebt sich zu einer schweigeminute.)', 'frau präsidentin, zur geschäftsordnung.', 'wie sie sicher aus der presse und dem fernsehen wissen, gab es in sri lanka mehrere bombenexplosionen mit zahlreichen toten.', 'zu den attentatsopfern, die es in jüngster zeit in sri lanka zu beklagen gab, zählt auch herr kumar ponnambalam, der dem europäischen parlament erst vor wenigen monaten einen besuch abgestattet hatte.']\n"
     ]
    }
   ],
   "source": [
    "# english\n",
    "[english_lowercase_lines, _] = preprocess(lines_english)\n",
    "print(english_lowercase_lines.take(10))\n",
    "print(\"----------------------------------------------\")\n",
    "# original language\n",
    "[de_lowercase_lines, _] = preprocess(lines_de)\n",
    "print(de_lowercase_lines.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994b9d7-429b-4e6a-b882-5336ea540f95",
   "metadata": {},
   "source": [
    "A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efcf7d2-3069-4bee-bae6-9a81c2207554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# english\n",
    "lines_english1 = english_lowercase_lines.map(lambda line: line.split('\\n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c5cd08-c43c-4aa8-a6b5-93261f58fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# original \n",
    "lines_de1 = de_lowercase_lines.map(lambda line: line.split('\\n'))\n",
    "line_de_counts = lines_de1.map(lambda w: len(w))\n",
    "total_de_lines = line_de_counts.reduce(add)\n",
    "print(f'total number of lines = {total_de_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482df70-98d6-4033-ad56-d9b78809bac3",
   "metadata": {},
   "source": [
    "A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "\n",
    "After verification, the line counts are exactly the same as it is before preprocessing.\n",
    "\n",
    "Total number of lines = 1920209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c38dcc-9a41-4e69-afec-d0dc718c6a64",
   "metadata": {},
   "source": [
    "## Question1 A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3218f-9e63-49d5-87d5-274d7ef93659",
   "metadata": {},
   "source": [
    "A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "306bfb62-a4fc-4214-8afe-a0aaf7669e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3663193), ('of', 1736975), ('to', 1611788), ('and', 1345073), ('in', 1134026), ('that', 835874), ('a', 810540), ('is', 792564), ('for', 557349), ('we', 551244)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# english\n",
    "[_, english_words] = preprocess(lines_english)\n",
    "english_word_key = english_words.map(lambda w: w.strip()).map(lambda w: (w, 1))\n",
    "english_word_counts = english_word_key.reduceByKey(add)\n",
    "print(english_word_counts.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "287f2fc1-e659-4335-b8a4-205922d21b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('die', 1980477), ('der', 1710353), ('und', 1337721), ('in', 781362), ('zu', 618872), ('den', 577654), ('wir', 489036), ('für', 478326), ('ich', 469025), ('das', 466127)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# original \n",
    "[_, de_words] = preprocess(lines_de)\n",
    "de_word_key = de_words.map(lambda w: w.strip()).map(lambda w: (w, 1))\n",
    "de_word_counts = de_word_key.reduceByKey(add)\n",
    "print(de_word_counts.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea4bf10e-0592-4338-b94e-226adc33cae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resumption', 1), ('of', 1), ('the', 1), ('session', 1), ('i', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words.take(3)\n",
    "english_word_key.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b694f65-c934-492c-b910-112a0eba7ec1",
   "metadata": {},
   "source": [
    "A.3.2 Verify that your results are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e39b0-4470-441b-ad1e-3a8f40fb5b4a",
   "metadata": {},
   "source": [
    "The pipeline to get the 10 most frequently according words:\n",
    "\n",
    "1. get the splited words using the 'preprocess' function, e.g ['resumption', 'of', 'the']\n",
    "2. map step: remove the extra blank space and make a key-value-pair, e.g [('resumption', 1), ('of', 1), ('the', 1)]\n",
    "3. reduce step: combine the pairs with the same key, add up the corresponding value, e.g ('of', 1), ('of', 1) --> ('of', 2).\n",
    "4. output the ordered result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71941-8e24-4442-bee0-a64f83eeb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example #1 - Filter by Top_level Domain and compute most common words ##\n",
    "\n",
    "# Try .ac.uk, .ru, .se, .com\n",
    "p = re.compile('WARC-Target-URI: \\S+\\.ac.uk', re.IGNORECASE)\n",
    "\n",
    "\n",
    "# Note: .partition(..) returns a 3-tuple: the string before the separator (index 0), \n",
    "# the separotor (index 1), and the part of the string afterwards (index 2) -- which is the part we want.\n",
    "all_words = rdd\\\n",
    "    .filter(lambda doc: bool(p.search(doc[1])))\\\n",
    "    .map(lambda web_text: web_text[1].partition('\\r\\n\\r\\n')[2])\\\n",
    "    .flatMap(lambda t: t.split(' '))\\\n",
    "    .flatMap(lambda w: w.split('\\n'))\\\n",
    "\n",
    "\n",
    "\n",
    "all_words_and_count = all_words.map(lambda w: w.strip())\\\n",
    "    .map(lambda w: (w,1))\n",
    "\n",
    "\n",
    "word_counts = all_words_and_count.reduceByKey(add)\n",
    "\n",
    "print(word_counts.takeOrdered(60, key=lambda x: -x[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
