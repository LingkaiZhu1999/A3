{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e10ade2-fd38-4dcf-a592-450fa3c1fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/20 19:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/20 19:11:28 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.119:7077\") \\\n",
    "        .appName(\"GZ\")\\\n",
    "        .config(\"spark.executor.cores\",16)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfb345-e799-4a03-92fd-343700ae7a3b",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6c3b4-e27a-43b0-94e3-dba28eb862a8",
   "metadata": {},
   "source": [
    "## Question A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c71c0-e436-4559-8bf5-8038547177e3",
   "metadata": {},
   "source": [
    "A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a706d8-8974-446e-b60d-1edcf336faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "lines_english = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.en\")\n",
    "print(lines_english.first())\n",
    "lines_english1 = lines_english.map(lambda line: line.split('\\n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443148f-6d38-415f-8e13-0d8e7c610523",
   "metadata": {},
   "source": [
    "A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b397ad9-4b47-4f9a-8a96-ab8a440c3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiederaufnahme der Sitzungsperiode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines_de = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.de\")\n",
    "print(lines_de.first())\n",
    "lines_de1 = lines_de.map(lambda line: line.split('\\n'))\n",
    "line_de_counts = lines_de1.map(lambda w: len(w))\n",
    "total_de_lines = line_de_counts.reduce(add)\n",
    "print(f'total number of lines = {total_de_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad1568-f846-4f61-b29b-cc15b983ea0f",
   "metadata": {},
   "source": [
    "A.1.3 Verify that the line counts are the same for the two languages.\n",
    "In this case, the count of the english transcripts is 1920209, which is equal to its original language's text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d9a3f-4f26-48ee-95ae-8b9d8015d9d3",
   "metadata": {},
   "source": [
    "A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3b27d1-0f3e-43f2-950e-73aad506dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions of the english: 3\n",
      "number of partitions of the original: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"number of partitions of the english:\", lines_english.getNumPartitions())\n",
    "print(\"number of partitions of the original:\", lines_de.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb19ead-7e17-459a-ba1c-14e54af846bb",
   "metadata": {},
   "source": [
    "## Question A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be21ccd-ce6f-4b6b-88c8-fa1532a7f1a7",
   "metadata": {},
   "source": [
    "A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    "\n",
    "● Lowercase the text\n",
    "\n",
    "● Tokenize the text (split on space)\n",
    "\n",
    "Hint: define a function to run in your driver application to avoid writing this code twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d58de0d-3cfb-4821-ac4b-83ada3f46c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "def preprocess(lines):\n",
    "    lowercase_lines = lines.map(lambda line: line.lower())\n",
    "    words = lowercase_lines\\\n",
    "    .flatMap(lambda line: line.split(' '))\\\n",
    "    .flatMap(lambda line: line.split('\\n'))\n",
    "    return lowercase_lines, words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfa47b-0a9f-4dd9-bf7b-6237e93f72ce",
   "metadata": {},
   "source": [
    "A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd95ba21-b220-401a-a2ec-81a3328b30af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resumption of the session', 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'you have requested a debate on this subject in the course of the next few days, during this part-session.', \"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\", \"please rise, then, for this minute' s silence.\", \"(the house rose and observed a minute' s silence)\", 'madam president, on a point of order.', 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.', 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.']\n",
      "----------------------------------------------\n",
      "['wiederaufnahme der sitzungsperiode', 'ich erkläre die am freitag, dem 17. dezember unterbrochene sitzungsperiode des europäischen parlaments für wiederaufgenommen, wünsche ihnen nochmals alles gute zum jahreswechsel und hoffe, daß sie schöne ferien hatten.', 'wie sie feststellen konnten, ist der gefürchtete \"millenium-bug \" nicht eingetreten. doch sind bürger einiger unserer mitgliedstaaten opfer von schrecklichen naturkatastrophen geworden.', 'im parlament besteht der wunsch nach einer aussprache im verlauf dieser sitzungsperiode in den nächsten tagen.', 'heute möchte ich sie bitten - das ist auch der wunsch einiger kolleginnen und kollegen -, allen opfern der stürme, insbesondere in den verschiedenen ländern der europäischen union, in einer schweigeminute zu gedenken.', 'ich bitte sie, sich zu einer schweigeminute zu erheben.', '(das parlament erhebt sich zu einer schweigeminute.)', 'frau präsidentin, zur geschäftsordnung.', 'wie sie sicher aus der presse und dem fernsehen wissen, gab es in sri lanka mehrere bombenexplosionen mit zahlreichen toten.', 'zu den attentatsopfern, die es in jüngster zeit in sri lanka zu beklagen gab, zählt auch herr kumar ponnambalam, der dem europäischen parlament erst vor wenigen monaten einen besuch abgestattet hatte.']\n"
     ]
    }
   ],
   "source": [
    "# english\n",
    "[english_lowercase_lines, _] = preprocess(lines_english)\n",
    "print(english_lowercase_lines.take(10))\n",
    "print(\"----------------------------------------------\")\n",
    "# original language\n",
    "[de_lowercase_lines, _] = preprocess(lines_de)\n",
    "print(de_lowercase_lines.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994b9d7-429b-4e6a-b882-5336ea540f95",
   "metadata": {},
   "source": [
    "A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9efcf7d2-3069-4bee-bae6-9a81c2207554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# english\n",
    "lines_english1 = english_lowercase_lines.map(lambda line: line.split('\\n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c5cd08-c43c-4aa8-a6b5-93261f58fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# original \n",
    "lines_de1 = de_lowercase_lines.map(lambda line: line.split('\\n'))\n",
    "line_de_counts = lines_de1.map(lambda w: len(w))\n",
    "total_de_lines = line_de_counts.reduce(add)\n",
    "print(f'total number of lines = {total_de_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482df70-98d6-4033-ad56-d9b78809bac3",
   "metadata": {},
   "source": [
    "A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "\n",
    "After verification, the line counts are exactly the same as it is before preprocessing.\n",
    "\n",
    "Total number of lines = 1920209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c38dcc-9a41-4e69-afec-d0dc718c6a64",
   "metadata": {},
   "source": [
    "## Question1 A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3218f-9e63-49d5-87d5-274d7ef93659",
   "metadata": {},
   "source": [
    "A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306bfb62-a4fc-4214-8afe-a0aaf7669e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3663193), ('of', 1736975), ('to', 1611788), ('and', 1345073), ('in', 1134026), ('that', 835874), ('a', 810540), ('is', 792564), ('for', 557349), ('we', 551244)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# english\n",
    "[_, english_words] = preprocess(lines_english)\n",
    "english_word_key = english_words.map(lambda w: w.strip()).map(lambda w: (w, 1))\n",
    "english_word_counts = english_word_key.reduceByKey(add)\n",
    "print(english_word_counts.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287f2fc1-e659-4335-b8a4-205922d21b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('die', 1980477), ('der', 1710353), ('und', 1337721), ('in', 781362), ('zu', 618872), ('den', 577654), ('wir', 489036), ('für', 478326), ('ich', 469025), ('das', 466127)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# original \n",
    "[_, de_words] = preprocess(lines_de)\n",
    "de_word_key = de_words.map(lambda w: w.strip()).map(lambda w: (w, 1))\n",
    "de_word_counts = de_word_key.reduceByKey(add)\n",
    "print(de_word_counts.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4bf10e-0592-4338-b94e-226adc33cae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resumption', 1), ('of', 1), ('the', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words.take(3)\n",
    "english_word_key.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b694f65-c934-492c-b910-112a0eba7ec1",
   "metadata": {},
   "source": [
    "A.3.2 Verify that your results are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e39b0-4470-441b-ad1e-3a8f40fb5b4a",
   "metadata": {},
   "source": [
    "The pipeline to get the 10 most frequently according words:\n",
    "\n",
    "1. get the splited words using the 'preprocess' function, e.g ['resumption', 'of', 'the']\n",
    "2. map step: remove the extra blank space and make a key-value-pair, e.g [('resumption', 1), ('of', 1), ('the', 1)]\n",
    "3. reduce step: combine the pairs with the same key, add up the corresponding value, e.g ('of', 1), ('of', 1) --> ('of', 2).\n",
    "4. output the ordered result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa4305-39fb-4f19-b95c-6cdcccf6d0c2",
   "metadata": {},
   "source": [
    "## Question A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904fd04-18ad-461d-a30c-0411661bfed7",
   "metadata": {},
   "source": [
    "A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated.\n",
    "\n",
    "Follow this approach. Work with the pair of RDDs you created in question A.2.\n",
    "Hint: make a new pair of RDDs for each step, sv_1, en_1, sv_2, en_2, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bade6b2-4330-455d-b236-7c2391a547f5",
   "metadata": {},
   "source": [
    "1. Key the lines by their line number (hint: ZipWithIndex()).\n",
    "2. Swap the key and value - so that the line number is the key.\n",
    "3. Join the two RDDs together according to the line number key, so you have pairs of matching lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fc4e8c3-49c1-4c2a-a703-7bd9bed0aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'resumption of the session'), (1, 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'), (2, \"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"), (3, 'you have requested a debate on this subject in the course of the next few days, during this part-session.'), (4, \"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\"), (5, \"please rise, then, for this minute' s silence.\"), (6, \"(the house rose and observed a minute' s silence)\"), (7, 'madam president, on a point of order.'), (8, 'you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.'), (9, 'one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'wiederaufnahme der sitzungsperiode'), (1, 'ich erkläre die am freitag, dem 17. dezember unterbrochene sitzungsperiode des europäischen parlaments für wiederaufgenommen, wünsche ihnen nochmals alles gute zum jahreswechsel und hoffe, daß sie schöne ferien hatten.'), (2, 'wie sie feststellen konnten, ist der gefürchtete \"millenium-bug \" nicht eingetreten. doch sind bürger einiger unserer mitgliedstaaten opfer von schrecklichen naturkatastrophen geworden.'), (3, 'im parlament besteht der wunsch nach einer aussprache im verlauf dieser sitzungsperiode in den nächsten tagen.'), (4, 'heute möchte ich sie bitten - das ist auch der wunsch einiger kolleginnen und kollegen -, allen opfern der stürme, insbesondere in den verschiedenen ländern der europäischen union, in einer schweigeminute zu gedenken.'), (5, 'ich bitte sie, sich zu einer schweigeminute zu erheben.'), (6, '(das parlament erhebt sich zu einer schweigeminute.)'), (7, 'frau präsidentin, zur geschäftsordnung.'), (8, 'wie sie sicher aus der presse und dem fernsehen wissen, gab es in sri lanka mehrere bombenexplosionen mit zahlreichen toten.'), (9, 'zu den attentatsopfern, die es in jüngster zeit in sri lanka zu beklagen gab, zählt auch herr kumar ponnambalam, der dem europäischen parlament erst vor wenigen monaten einen besuch abgestattet hatte.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# english\n",
    "[english_lowercase_lines, _] = preprocess(lines_english)\n",
    "english_lines_index = english_lowercase_lines.zipWithIndex()\n",
    "english_index_lines = english_lines_index.map(lambda x: (x[1], x[0]))\n",
    "print(english_index_lines.take(10))\n",
    "\n",
    "# original\n",
    "[de_lowercase_lines, _] = preprocess(lines_de)\n",
    "de_lines_index = de_lowercase_lines.zipWithIndex()\n",
    "de_index_lines = de_lines_index.map(lambda x: (x[1], x[0]))\n",
    "print(de_index_lines.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63bad72c-b3ce-4b06-a41c-cbf36211cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 18:10:45 ERROR TaskSetManager: Task 5 in stage 38.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 38.0 failed 4 times, most recent failure: Lost task 5.3 in stage 38.0 (TID 158) (192.168.2.119 executor 1): com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:188)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:164)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:269)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:283)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:243)\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:186)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:188)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:164)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:269)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:283)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:243)\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:186)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# join two rdds\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# de_en_join = de_index_lines.join(english_index_lines).map(lambda x: (x[1][0], x[1][1]))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# de_en_exclude_empty = de_en_join.filter(lambda x: x[0]!=0 & x[1]!=0)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# de_en_exclude_empty.first()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# (rdd1 union rdd2).reduceByKey(_ ++ _)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m de_en_join \u001b[38;5;241m=\u001b[39m de_index_lines\u001b[38;5;241m.\u001b[39munion(english_index_lines)\u001b[38;5;241m.\u001b[39mreduceByKey(add)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mde_en_join\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1568\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1567\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1568\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1570\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1571\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:1227\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;66;03m# Implementation note: This is implemented as a mapPartitions followed\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# by runJob() in order to avoid having to pass a Python lambda into\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;66;03m# SparkContext#runJob.\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m-> 1227\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 38.0 failed 4 times, most recent failure: Lost task 5.3 in stage 38.0 (TID 158) (192.168.2.119 executor 1): com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:188)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:164)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:269)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:283)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:243)\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:186)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor56.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:188)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:164)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:269)\n\tat org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:283)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:243)\n\tat com.esotericsoftware.kryo.io.Output.flush(Output.java:186)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# join two rdds\n",
    "# de_en_join = de_index_lines.join(english_index_lines).map(lambda x: (x[1][0], x[1][1]))\n",
    "# de_en_exclude_empty = de_en_join.filter(lambda x: x[0]!=0 & x[1]!=0)\n",
    "# de_en_exclude_empty.first()\n",
    "# (rdd1 union rdd2).reduceByKey(_ ++ _)\n",
    "de_en_join = de_index_lines.union(english_index_lines).reduceByKey(add)\n",
    "de_en_join.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51076b-fa27-49af-9f37-87239adadd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560174f-5c52-4e06-9797-7bf5bc125626",
   "metadata": {},
   "outputs": [],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9a139-5b0c-42d7-ae53-769e45fa657a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71941-8e24-4442-bee0-a64f83eeb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example #1 - Filter by Top_level Domain and compute most common words ##\n",
    "\n",
    "# Try .ac.uk, .ru, .se, .com\n",
    "p = re.compile('WARC-Target-URI: \\S+\\.ac.uk', re.IGNORECASE)\n",
    "\n",
    "\n",
    "# Note: .partition(..) returns a 3-tuple: the string before the separator (index 0), \n",
    "# the separotor (index 1), and the part of the string afterwards (index 2) -- which is the part we want.\n",
    "all_words = rdd\\\n",
    "    .filter(lambda doc: bool(p.search(doc[1])))\\\n",
    "    .map(lambda web_text: web_text[1].partition('\\r\\n\\r\\n')[2])\\\n",
    "    .flatMap(lambda t: t.split(' '))\\\n",
    "    .flatMap(lambda w: w.split('\\n'))\\\n",
    "\n",
    "\n",
    "\n",
    "all_words_and_count = all_words.map(lambda w: w.strip())\\\n",
    "    .map(lambda w: (w,1))\n",
    "\n",
    "\n",
    "word_counts = all_words_and_count.reduceByKey(add)\n",
    "\n",
    "print(word_counts.takeOrdered(60, key=lambda x: -x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369287fa-57e4-4e87-826c-87d12d950729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
