{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e10ade2-fd38-4dcf-a592-450fa3c1fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/16 18:15:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/16 18:15:19 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.119:7077\") \\\n",
    "        .appName(\"LingkaiZhu\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfb345-e799-4a03-92fd-343700ae7a3b",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c71c0-e436-4559-8bf5-8038547177e3",
   "metadata": {},
   "source": [
    "A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a706d8-8974-446e-b60d-1edcf336faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumption of the session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines_english = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.en\")\n",
    "print(lines_english.first())\n",
    "lines_english1 = lines_english.map(lambda line: line.split('/n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443148f-6d38-415f-8e13-0d8e7c610523",
   "metadata": {},
   "source": [
    "A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b397ad9-4b47-4f9a-8a96-ab8a440c3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiederaufnahme der Sitzungsperiode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of lines = 1920225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines_sv = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.de\")\n",
    "print(lines_sv.first())\n",
    "lines_sv1 = lines_sv.map(lambda line: line.split('/n'))\n",
    "line_sv_counts = lines_sv1.map(lambda w: len(w))\n",
    "total_sv_lines = line_sv_counts.reduce(add)\n",
    "print(f'total number of lines = {total_sv_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad1568-f846-4f61-b29b-cc15b983ea0f",
   "metadata": {},
   "source": [
    "A.1.3 Verify that the line counts are the same for the two languages.\n",
    "In this case, the count of the english transcripts is 1920225, which is equal to its original language's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de5770-daca-4e54-bb1f-4d150849460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb3b27d1-0f3e-43f2-950e-73aad506dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions of the english: 3\n",
      "number of partitions of the original: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"number of partitions of the english:\", lines_english.getNumPartitions())\n",
    "print(\"number of partitions of the original:\", lines_sv.getNumPartitions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
