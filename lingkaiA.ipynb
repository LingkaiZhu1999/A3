{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e10ade2-fd38-4dcf-a592-450fa3c1fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/19 14:23:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/19 14:23:21 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.119:7077\") \\\n",
    "        .appName(\"LingkaiZhu\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfb345-e799-4a03-92fd-343700ae7a3b",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6c3b4-e27a-43b0-94e3-dba28eb862a8",
   "metadata": {},
   "source": [
    "## Question A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c71c0-e436-4559-8bf5-8038547177e3",
   "metadata": {},
   "source": [
    "A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a706d8-8974-446e-b60d-1edcf336faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "lines_english = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.en\")\n",
    "print(lines_english.first())\n",
    "lines_english1 = lines_english.map(lambda line: line.split('\\n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443148f-6d38-415f-8e13-0d8e7c610523",
   "metadata": {},
   "source": [
    "A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b397ad9-4b47-4f9a-8a96-ab8a440c3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lines_de \u001b[38;5;241m=\u001b[39m \u001b[43mspark_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.de\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(lines_de\u001b[38;5;241m.\u001b[39mfirst())\n\u001b[1;32m      3\u001b[0m lines_de1 \u001b[38;5;241m=\u001b[39m lines_de\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:654\u001b[0m, in \u001b[0;36mSparkContext.textFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtextFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, minPartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    Read a text file from HDFS, a local file system (available on all\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    nodes), or any Hadoop-supported file system URI, and return it as an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    ['Hello world!']\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     minPartitions \u001b[38;5;241m=\u001b[39m minPartitions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39mtextFile(name, minPartitions), \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    656\u001b[0m                UTF8Deserializer(use_unicode))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:450\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    reduce tasks)\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1308\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1310\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lines_de = spark_context.textFile(\"hdfs://192.168.2.119:9000/europarl/europarl-v7.de-en.de\")\n",
    "print(lines_de.first())\n",
    "lines_de1 = lines_de.map(lambda line: line.split('\\n'))\n",
    "line_de_counts = lines_de1.map(lambda w: len(w))\n",
    "total_de_lines = line_de_counts.reduce(add)\n",
    "print(f'total number of lines = {total_de_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad1568-f846-4f61-b29b-cc15b983ea0f",
   "metadata": {},
   "source": [
    "A.1.3 Verify that the line counts are the same for the two languages.\n",
    "In this case, the count of the english transcripts is 1920209, which is equal to its original language's text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d9a3f-4f26-48ee-95ae-8b9d8015d9d3",
   "metadata": {},
   "source": [
    "A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b27d1-0f3e-43f2-950e-73aad506dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of partitions of the english:\", lines_english.getNumPartitions())\n",
    "print(\"number of partitions of the original:\", lines_de.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb19ead-7e17-459a-ba1c-14e54af846bb",
   "metadata": {},
   "source": [
    "## Question A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be21ccd-ce6f-4b6b-88c8-fa1532a7f1a7",
   "metadata": {},
   "source": [
    "A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    "\n",
    "● Lowercase the text\n",
    "\n",
    "● Tokenize the text (split on space)\n",
    "\n",
    "Hint: define a function to run in your driver application to avoid writing this code twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58de0d-3cfb-4821-ac4b-83ada3f46c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "def preprocess(lines):\n",
    "    lowercase_lines = lines.map(lambda line: line.lower())\n",
    "    words = lowercase_lines\\\n",
    "    .flatMap(lambda line: line.split(' '))\\\n",
    "    .flatMap(lambda line: line.split('\\n'))\n",
    "    return lowercase_lines, words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfa47b-0a9f-4dd9-bf7b-6237e93f72ce",
   "metadata": {},
   "source": [
    "A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95ba21-b220-401a-a2ec-81a3328b30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english\n",
    "[english_lowercase_lines, _] = preprocess(lines_english)\n",
    "print(english_lowercase_lines.take(10))\n",
    "print(\"----------------------------------------------\")\n",
    "# original language\n",
    "[de_lowercase_lines, _] = preprocess(lines_de)\n",
    "print(de_lowercase_lines.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994b9d7-429b-4e6a-b882-5336ea540f95",
   "metadata": {},
   "source": [
    "A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efcf7d2-3069-4bee-bae6-9a81c2207554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english\n",
    "lines_english1 = english_lowercase_lines.map(lambda line: line.split('\\n'))\n",
    "line_english_counts = lines_english1.map(lambda w: len(w))\n",
    "total_english_lines = line_english_counts.reduce(add)\n",
    "print(f'total number of lines = {total_english_lines}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5cd08-c43c-4aa8-a6b5-93261f58fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "lines_de1 = de_lowercase_lines.map(lambda line: line.split('\\n'))\n",
    "line_de_counts = lines_de1.map(lambda w: len(w))\n",
    "total_de_lines = line_de_counts.reduce(add)\n",
    "print(f'total number of lines = {total_de_lines}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482df70-98d6-4033-ad56-d9b78809bac3",
   "metadata": {},
   "source": [
    "A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "\n",
    "After verification, the line counts are exactly the same as it is before preprocessing.\n",
    "\n",
    "Total number of lines = 1920209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c38dcc-9a41-4e69-afec-d0dc718c6a64",
   "metadata": {},
   "source": [
    "## Question1 A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3218f-9e63-49d5-87d5-274d7ef93659",
   "metadata": {},
   "source": [
    "A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306bfb62-a4fc-4214-8afe-a0aaf7669e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english\n",
    "[_, english_words] = preprocess(lines_english)\n",
    "english_word_key = english_words.map(lambda w: w.strip()).map(lambda w: (w, 1))\n",
    "english_word_counts = english_word_key.reduceByKey(add)\n",
    "print(english_word_counts.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f2fc1-e659-4335-b8a4-205922d21b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "[_, de_words] = preprocess(lines_de)\n",
    "de_word_key = de_words.map(lambda w: w.strip()).map(lambda w: (w, 1))\n",
    "de_word_counts = de_word_key.reduceByKey(add)\n",
    "print(de_word_counts.takeOrdered(10, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bf10e-0592-4338-b94e-226adc33cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words.take(3)\n",
    "english_word_key.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b694f65-c934-492c-b910-112a0eba7ec1",
   "metadata": {},
   "source": [
    "A.3.2 Verify that your results are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e39b0-4470-441b-ad1e-3a8f40fb5b4a",
   "metadata": {},
   "source": [
    "The pipeline to get the 10 most frequently according words:\n",
    "\n",
    "1. get the splited words using the 'preprocess' function, e.g ['resumption', 'of', 'the']\n",
    "2. map step: remove the extra blank space and make a key-value-pair, e.g [('resumption', 1), ('of', 1), ('the', 1)]\n",
    "3. reduce step: combine the pairs with the same key, add up the corresponding value, e.g ('of', 1), ('of', 1) --> ('of', 2).\n",
    "4. output the ordered result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa4305-39fb-4f19-b95c-6cdcccf6d0c2",
   "metadata": {},
   "source": [
    "## Question A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904fd04-18ad-461d-a30c-0411661bfed7",
   "metadata": {},
   "source": [
    "A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated.\n",
    "\n",
    "Follow this approach. Work with the pair of RDDs you created in question A.2.\n",
    "Hint: make a new pair of RDDs for each step, sv_1, en_1, sv_2, en_2, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bade6b2-4330-455d-b236-7c2391a547f5",
   "metadata": {},
   "source": [
    "1. Key the lines by their line number (hint: ZipWithIndex()).\n",
    "2. Swap the key and value - so that the line number is the key.\n",
    "3. Join the two RDDs together according to the line number key, so you have pairs of matching lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4e8c3-49c1-4c2a-a703-7bd9bed0aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english\n",
    "[english_lowercase_lines, _] = preprocess(lines_english)\n",
    "english_lines_index = english_lowercase_lines.zipWithIndex()\n",
    "english_index_lines = english_lines_index.map(lambda x: (x[1], x[0]))\n",
    "#print(english_index_lines.take(10))\n",
    "\n",
    "# original\n",
    "[de_lowercase_lines, _] = preprocess(lines_de)\n",
    "de_lines_index = de_lowercase_lines.zipWithIndex()\n",
    "de_index_lines = de_lines_index.map(lambda x: (x[1], x[0]))\n",
    "#print(de_index_lines.take(10))\n",
    "\n",
    "# join two rdds\n",
    "de_en_join = de_index_lines.join(english_index_lines)\n",
    "de_en_join.map(lambda x: (x[1][0], x[1][1])).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560174f-5c52-4e06-9797-7bf5bc125626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c9a139-5b0c-42d7-ae53-769e45fa657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('resumption of the session', 0),\n",
       " ('i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       "  1),\n",
       " (\"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
       "  2),\n",
       " ('you have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
       "  3),\n",
       " (\"in the meantime, i should like to observe a minute' s silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\",\n",
       "  4),\n",
       " (\"please rise, then, for this minute' s silence.\", 5),\n",
       " (\"(the house rose and observed a minute' s silence)\", 6),\n",
       " ('madam president, on a point of order.', 7),\n",
       " ('you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.',\n",
       "  8),\n",
       " ('one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.',\n",
       "  9)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf71941-8e24-4442-bee0-a64f83eeb50a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Example #1 - Filter by Top_level Domain and compute most common words ##\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Try .ac.uk, .ru, .se, .com\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWARC-Target-URI: \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.ac.uk\u001b[39m\u001b[38;5;124m'\u001b[39m, re\u001b[38;5;241m.\u001b[39mIGNORECASE)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Note: .partition(..) returns a 3-tuple: the string before the separator (index 0), \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# the separotor (index 1), and the part of the string afterwards (index 2) -- which is the part we want.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m all_words \u001b[38;5;241m=\u001b[39m rdd\\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m doc: \u001b[38;5;28mbool\u001b[39m(p\u001b[38;5;241m.\u001b[39msearch(doc[\u001b[38;5;241m1\u001b[39m])))\\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m web_text: web_text[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m w: w\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\\\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "## Example #1 - Filter by Top_level Domain and compute most common words ##\n",
    "\n",
    "# Try .ac.uk, .ru, .se, .com\n",
    "p = re.compile('WARC-Target-URI: \\S+\\.ac.uk', re.IGNORECASE)\n",
    "\n",
    "\n",
    "# Note: .partition(..) returns a 3-tuple: the string before the separator (index 0), \n",
    "# the separotor (index 1), and the part of the string afterwards (index 2) -- which is the part we want.\n",
    "all_words = rdd\\\n",
    "    .filter(lambda doc: bool(p.search(doc[1])))\\\n",
    "    .map(lambda web_text: web_text[1].partition('\\r\\n\\r\\n')[2])\\\n",
    "    .flatMap(lambda t: t.split(' '))\\\n",
    "    .flatMap(lambda w: w.split('\\n'))\\\n",
    "\n",
    "\n",
    "\n",
    "all_words_and_count = all_words.map(lambda w: w.strip())\\\n",
    "    .map(lambda w: (w,1))\n",
    "\n",
    "\n",
    "word_counts = all_words_and_count.reduceByKey(add)\n",
    "\n",
    "print(word_counts.takeOrdered(60, key=lambda x: -x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369287fa-57e4-4e87-826c-87d12d950729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
